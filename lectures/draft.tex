\begin{frame}{Finding the autocorrelation}
To find the parameters $a_1, \ldots, a_N$, we need to compute the autocorrelation $r[0], \ldots, r[N-1]$ from $M+1$ samples of the signal $s[n]$.

Two methods:
\begin{enumerate}
	\item \textbf{Autocorrelation method}
	This method assumes that the signal $s[n]$ is \underline{zero outside the interval} $[0, M+1]$. Therefore, for $s[n]$ deterministic we have
	\begin{equation*}
	r_{ss}[|m|] = \sum_{n =0}^{M-|m|}s[n]s[n+|m|], |m| \leq N-1
	\end{equation*}
	This implies $r_{ss}[m] = 0, |m| > M$.
	\item \textbf{Covariance method}
	This method assumes that the signal $s[n]$ is \underline{unknown outside the interval} $[0, M+1]$. Therefore, for $s[n]$ deterministic we have
	\begin{equation*}
	r_{ss}[m] = \sum_{n = N}^{M}s[n]s[n+|m|], m = 0, 1, \ldots, N-1
	\end{equation*}	
\end{enumerate}
\end{frame}

\begin{frame}{Convolution as a matrix-vector product}
We can write a convolution as a matrix-vector product
\begin{align*}
y[n] &= \sum_{m = 0}^N h_nx[n-m] \\
y &= Xh \\
\begin{bmatrix}
y[0] \\
y[1] \\
y[2] \\
\vdots \\ 
y[L-1] \\
\end{bmatrix}
&= \begin{bmatrix}
x[0] & 0 & \ldots & 0 \\
x[1] & x[0] & \ddots & \vdots \\
\vdots & x[1] & \ddots &  0\\
x[L-1] & \vdots &\ddots &  x[0]\\
0 & x[L-1] & \ddots & x[1] \\
\vdots & \vdots & \ddots &  \vdots \\
0  & 0 & \ldots & x[L-1] 
\end{bmatrix}_{L\times N}\cdot\begin{bmatrix}
h_0 \\
h_1 \\
h_2 \\
\vdots \\ 
h_{N-1} \\
\end{bmatrix}
\end{align*}

The matrix $X$ is called \textbf{Toeplitz matrix}.

\vspace{0.25cm}
In Matlab: \texttt{>> X = toeplitz(c, r)}, where the vector \texttt{c} is the first column of $X$, and the vector \texttt{r} is the first row of $X$.

\end{frame}


\begin{frame}{All-pole model for deterministic signals}
Back to our problem when $s[n]$ is deterministic:
\begin{equation}
\sum_{n = 0}^{L-1} |v[n] - \sum_{m = 0}^N a_ns[n-m]|^2 \tag{if $s[n]$ is deterministic}
\end{equation}

Now we can write this in matrix notation
\begin{equation*}
||v - Sh||^2_2
\end{equation*}

This is a \textbf{least-squares problem}, and its solutions is 
\begin{equation}
h = S^{\dagger}v \tag{least-squares solution}
\end{equation}
where $S^{\dagger} = (S^TS)^{-1}S^T$ is the \textbf{Moore-Penrose pseudo-inverse}.

\vspace{0.25cm}
When $s[n]$ is deterministic, $v[n]$ is assumed to be an impulse. Hence, $v = [1, 0, \ldots, 0]^T$.
\end{frame}


\begin{frame}{All-pole model for random signals}
When $s[n]$ is random, we want to minimize
\begin{equation*}
\E\Big(|v[n] - \sum_{m = 0}^N a_ns[n-m]|^2\Big)
\end{equation*}



\begin{equation*}
\E(s[n]v[n]) = \sum_{m = 0}^N a_n\E(s[n]s[n-m]), \quad n = 0, \ldots, L-1
\end{equation*}

\end{frame}



%
\begin{frame}{Revisiting the Bartlett or triangular window}
\begin{columns}
	\begin{column}{0.5\textwidth}
		\textbf{Time}
		\begin{equation*}
		w_B[n] = \begin{cases}
		1 - |n|/M, & |n| < M \\
		0, & \text{otherwise}
		\end{cases}
		\end{equation*}
	\end{column}
	
	\begin{column}{0.5\textwidth}
		\textbf{Frequency}
		\begin{equation*}
		W_B[n] = \frac{1}{M}\bigg(\frac{\sin(\omega M/2)}{\sin(\omega/2)}\bigg)^2
		\end{equation*}
	\end{column}
\end{columns}

\hspace*{-0.75cm}\resizebox{0.95\paperwidth}{!}{\input{figs/barlett_time_freq.tex}}
\end{frame}


%
\begin{frame}{Design specifications}
Specifications of \textbf{frequency-selective} filters
\begin{center}
	\resizebox{0.7\linewidth}{!}{\input{figs/design_specs.tex}}
\end{center}

\textbf{Terminology}
\begin{itemize}
	\item $\delta_1$ passband ripple
	\item $\delta_2$ stopband ripple
	\item $\omega_p$ passband edge frequency
	\item $\omega_p$ stopband edge frequency
\end{itemize}
\end{frame}

\begin{frame}{Poles, zeros, and the frequency response}

\textbf{Adding one more zero:} $H(z) = (1 - r_1e^{j\theta}z^{-1})(1 - r_2e^{j\psi}z^{-1})$

\begin{columns}[t]
	\begin{column}{0.7\textwidth}
		\vspace{-0.5cm}
		\begin{center}
			\resizebox{\linewidth}{!}{\input{figs/pole_zero_freq_resp_2zeros.tex}}
		\end{center}
	\end{column}
	
	\begin{column}{0.3\textwidth}	
		\only<1|handout:1>{
			\textbf{Magnitude}
			\flushleft
			\begin{align*}
			H(z) &= \frac{z - r_1e^{j\theta}}{z}\cdot\frac{z - r_2e^{j\psi}}{z} \\
			|H(e^{j\omega})| &= \bigg|\frac{{\color{red2} v_1} - {\color{black} v_2}}{{\color{red2} v_1}}\bigg|\bigg|\frac{{\color{red2} v_1} - {\color{black} v_4}}{{\color{red2} v_1}}\bigg|\\
			&= \frac{||{\color{blue2} v_3}||}{||{\color{red2} v_1}||}\frac{||{\color{green2} v_5}||}{||{\color{red2} v_1}||} \\
			&= ||{\color{blue2} v_3}||||{\color{green2} v_5}||
			\end{align*}
		}
		
		\only<2|handout:2>{
			\textbf{Phase}
			\flushleft
			\begin{align*}
			H(z) &= \frac{z - r_1e^{j\theta}}{z}\cdot\frac{z - r_2e^{j\psi}}{z} \\
			\angle H(e^{j\omega})  &= \angle\frac{{\color{red2} v_1} - {\color{black} v_2}}{{\color{red2} v_1}} + \angle\frac{{\color{red2} v_1} - {\color{black} v_4}}{{\color{red2} v_1}}\\
			&= \angle {\color{blue2} v_3} + \angle {\color{green2} v_5} - 2\omega
			\end{align*}
		}
	\end{column}
\end{columns}

\only<1|handout:1>{\textbf{Conclusion:} As $e^{j\omega}$ approaches a zero, $|H(e^{j\omega})| \to 0$}
\only<2|handout:2>{\textbf{Conclusion:} A zero causes a negative to positive phase change of the phase response (\textbf{phase advance})}
\end{frame}



%
\begin{frame}{The unilateral $z$-transform}

The bilateral $z$-transform we've seen so far does not account for initial conditions.

\begin{block}{The unilateral $z$-transform}
	\begin{equation*} \tag{Direct transform}
	X^+(z) = \sum_{\tikz[baseline]{
			\node[fill=blue!20,anchor=base] (t1) {$n = 0$};
	}}^{\infty} x[n]z^{-n} 
	\end{equation*}
\end{block}

\begin{itemize}
	\item For causal signals, $X(z) = X^+(z)$. \\
	\item For any signal $y[n] \Longleftrightarrow Y(z)$, we have $y[n]u[n] \Longleftrightarrow Y^+(z)$. \\
	\item As the unilateral $z$-transform only takes into account the causal part of the signal, the ROC will always the exterior of a circle (ROC = $\{|z|>|r|\}$). Hence, there's no ambiguity.
\end{itemize}

\end{frame}

\subsection{Sampling and reconstruction}
%
\begin{frame}{Families of $z$-transforms}	
\begin{block}{Finite-length sequences}
	When $A(z) = 1$, we're left with $z$-transforms of the form
	\begin{align*}
	X(z) = b_0 + b_1z^{-1}+\ldots+b_Mz^{-M}
	\end{align*}
	\begin{itemize}
		\item These correspond to finite-length sequences or systems with \textbf{finite impulse response (FIR)}
		\item In this case, we have $M$ zeros, which are the roots of $X(z)$, and we have $M$ poles at $z = 0$. As a result, the ROC is the entire $z$-plane with the exception of $z = 0$, and possible poles at infinity i.e., $X(p = \infty) = \infty$.
		\item Since finite-length sequences are always absolute summable, the ROC of their $z$-trasnform is the entire $z$ plan, with the exception of $z = 0$ or $z = \infty$ (the poles).
	\end{itemize}	
\end{block}

\end{frame}

%
\begin{frame}{Obtaining frequency response from difference equation}
	
	\begin{block}{Difference equation}
		\begin{equation*}
		\sum_{k=0}^N a_k y[n-k] = \sum_{k=0}^Mb_k x[n-k]
		\end{equation*}
	\end{block}
	
	Using the linearity and time shift properties of the DTFT it follows that
	
	\begin{align*}
	\sum_{k=0}^N a_kY(e^{j\omega})e^{-j\omega k} = \sum_{k=0}^M b_kX(e^{j\omega})e^{-j\omega k} \\
	H(e^{j\omega}) = \frac{Y(e^{j\omega})}{X(e^{j\omega})} = \frac{\sum_{k=0}^M b_ke^{-j\omega k}}{\sum_{k=0}^N a_ke^{-j\omega k}}
	\end{align*}
\end{frame}


%%%%%%% Discrete Fourier Transform

